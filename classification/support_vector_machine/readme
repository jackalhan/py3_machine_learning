What is Support Vector Machine?
It worked better than Artificial Neural Network in hand-written-recognition. It is still one of the best machine learning algorithm

It seperates only 2 groups at a time but that does not mean that it can not support to classify more than two class. Actually it is seperating
one group to the rest of the groups at a time.

Binary classifier, positive or negative. It is defining best separating hyperplane

In cartezian line, let say we have a vector with the values of [3,4]
from the origin to the point, if we draw the line, the direction of the line/arrow would be northeast.
And the magnitude of the vector ||A|| is sqrt of (3^2 + 4^2) = sqrt(25) = 5

::: Dot product between two vectors :::
vector A = [1,3]
vector B = [4,2]

A.B = (1 X 4) + (3 X 2) = 4 + 6 = 10 -> SCALAR VALUE

What is the formula behind SVM?
Decision boundary = Is a hyperplane between two different groups.

Unknown Vector U . Unknown Vector W + b >= 0 :
    Then it belongs to one group
Unknown Vector U .Unknown Vector W + b <= 0 :
    Then it belongs to other group
else
    Decision Boundary = Hyperplane

Unknown is actually a feature set.

How do we know these unknown values. We already know that we are going to find unknown vector V,
but what about vector W and b value. Since we know that ;
Vector X (- signed vector) . vector W + b = -1
Vector X (+ signed vector) . vector W + b = 1

SO OUR EQUATION ;
for + class Yi = 1  so;

    Yi (vector Xi . vector W + b) - 1 = 0

for - class Yi = -1  so;

    Yi (vector Xi . vector W + b) + 1 = 0

We want to maximize Width, (decisioun boundary) hyperplane between two hyperplanes;
Width = Vector X (- signed vector) - Vector X (+ signed vector) . vector W / || vector W ||

Since we know the requations, we are replacing aforementioned equations to the this Width formula and what we have is the following:

WIDTH = 2 / ||vector W||

In order to maximize the Width, we need to minimize the || vector W ||     (magnitude vector W)
How to minimize the || vector W ||, so in order to that, we want to replace || vector W || with the
1 / 2 x || vector W || ^ 2

b is generally used for BIAS.
Adding b means that ( + b ), moving hyperplane by the value of b.

Maximize the b, minimize the W
L (W, b) = 1/2 x (magnitude of vector W ) ^ 2 - sum of (alpha i x (Yi (vector Xi . vector W + b) - 1))

Differenciate @ L / @ W = vector W - sum of the (alpha i . Yi. Vector Xi)
Differenciate @ L / @ b = sum of the (alpha i . Yi. Vector Xi)

L = sum