REGRESSION
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Idea is to take continuous data and figure out a best-fit line to that data.
Basically, what that just boils down to is, you are trying to like model your data in the way that we do
that with regression is with a really simple linear regression is with just a straight line so the
eqauation of a line as well y = mx + b. To find out m and b is ?
For example, for the stock prices. Stock prices are cont. data so we have months and months of stock prices and
each prices its own kind of unique day but all the data is kind of one dataset together as oppoed like with
classification or each group of data has its own unique label so supervised machine learning for thi example, features are
the continuous data.

Once we start to use WIKI/GOOGL data, we need to grap some meaningful features from the dataset.
All columns of the dataset are the features but we need to play with the features that are meaningful for our idea.

Columns descriptions : Adjusted after things like stock splits. Stock split maybe you are a company of 10 stocks
each stock is a 1000 dollars share and you want to decide...I want to lower my stock price and want to increase stocks.
I want my ppl to be able to buy shares in my company for less than a thousand dollars so you might say OK, every shares
now two shares so we have 20 total shares and share prices five hundred dollars. You have adjused prices to like account
for that so does not look like price went from a thousand dollars of five hundred dollars that is adjusted.

You can not find/seek corelations or relationships between features with a regression. DL is a tool for that.

'Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume'
some of listed columns maybe worthless but they have some relationships. For example The margin between High and Low
can tell us a little bit volatility for the day. Another example, open price that is the starting price for the day and
its relationship to the clothes price tells us did the price go up/go down, if so by how much

Therefore, we need to define those relationships and feed regressions instead of redundant or useless attributes.
The followings are columns/attributes of our new calculated and created dataframe : 'Adj. Open', 'Adj. Close', 'New/Old Perc.', 'Adj. High', 'Adj. Low', 'High/Low Perc.', 'Adj. Volume'
But these are not label. What actually would be the label ? Somepoint in the future the price. .........

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
CUSTOM LINEAR REGRESSION  (2 line axis)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Y = mX + b

For best fit line
m (slope) :
    m = ((mean of X * mean of Y) - mean of (X * Y)) /  (mean of X)^2 - mean of (X^2)

y = intercept
    b = mean of Y - m * mean of X
-----------------------------------------------------------------------------------------
How accurate our regression / result is? Our best fit is really best?
    squarred error = the square of a distance from the point to the regression line.
    since the more the point is far from the regression line, the more error value it should have.

why are we quaring this error ?
    because, at some points distance can be negative, at some points distance can be positive therefore
    we are squarring it in order to always get the positive value. We want also to penalise the outliers
    therefore we do not use the absolute method. We can also use power of 4, 6, 18 in order to differentiate
    the heavy of panalising but it may not be standart.

Squarred Error :
y headline = best fit = regressionline (all the same)
    r^2 = 1 - (SE of y headline / SE of the mean of Y)

The aim is to have the possible high r^2 value.
For example, let r^2 = 0.8 ;

    0.8 = 1 - 0.2
    Therefore 0.2 belongs to 2 / 10 which means that
        SE of y headline  = 2
        SE of the mean of Y = 10

    SE of y headline is far away from the SE of the mean of y.

